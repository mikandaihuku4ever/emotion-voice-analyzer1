# ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„éŸ³å£°æ„Ÿæƒ…åˆ†æã®é¸æŠè‚¢

"""
Vokaturiã‚ˆã‚Šç²¾åº¦ãŒé«˜ã„å¯èƒ½æ€§ã®ã‚ã‚‹ä»£æ›¿æ‰‹æ®µï¼š

1. **Hugging Face Transformers - wav2vec2ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«**
   - ãƒ¢ãƒ‡ãƒ«: ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition
   - åˆ©ç‚¹: æœ€æ–°ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã€7ã¤ã®æ„Ÿæƒ…ã‚’èªè­˜
   - ç²¾åº¦: Vokaturiã‚ˆã‚Šé«˜ç²¾åº¦

2. **librosa + TensorFlow/PyTorch ãƒ¢ãƒ‡ãƒ«**
   - éŸ³éŸ¿ç‰¹å¾´é‡ï¼ˆMFCCã€ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ï¼‰ã‚’æŠ½å‡º
   - ã‚«ã‚¹ã‚¿ãƒ ãƒ¢ãƒ‡ãƒ«ã§å­¦ç¿’å¯èƒ½

3. **Microsoft Azure Speech Service**
   - å•†ç”¨ã ãŒé«˜ç²¾åº¦
   - å¤šè¨€èªå¯¾å¿œ

4. **æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«**
   - æ—¥æœ¬èªéŸ³å£°ã«ç‰¹åŒ–ã—ãŸæ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«
   - ä¾‹: JTES (Japanese Twitter Emotion Dataset) ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«
"""

import sys
import wave
import numpy as np
import os
import ctypes
import torch
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor

print("=" * 60)
print("ã€é«˜ç²¾åº¦ç‰ˆã€‘éŸ³å£°æ„Ÿæƒ…åˆ†æãƒ—ãƒ­ã‚°ãƒ©ãƒ ")
print("=" * 60)

# WAVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
file_name = "output.wav"
print(f"\nåˆ†æå¯¾è±¡: {file_name}")

# WAVãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
with wave.open(file_name, 'r') as wav_file:
    num_channels = wav_file.getnchannels()
    sample_rate = wav_file.getframerate()
    num_frames = wav_file.getnframes()
    
    print(f"\nğŸ“Š éŸ³å£°æƒ…å ±:")
    print(f"  - ãƒãƒ£ãƒ³ãƒãƒ«æ•°: {num_channels}")
    print(f"  - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: {sample_rate} Hz")
    print(f"  - é•·ã•: {num_frames / sample_rate:.2f} ç§’")
    
    # éŸ³å£°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
    buffer = wav_file.readframes(num_frames)
    
    # NumPyé…åˆ—ã«å¤‰æ›
    if num_channels == 1:
        audio_data = np.frombuffer(buffer, dtype=np.int16).astype(np.float32) / 32768.0
    else:
        audio_data = np.frombuffer(buffer, dtype=np.int16).astype(np.float32) / 32768.0
        audio_data = audio_data.reshape(-1, num_channels).mean(axis=1)

print("\nğŸ”„ æ„Ÿæƒ…åˆ†æãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")

# Wav2Vec2ãƒ™ãƒ¼ã‚¹ã®æ„Ÿæƒ…èªè­˜ãƒ¢ãƒ‡ãƒ«
model_name = "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"

try:
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
    model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®šï¼ˆGPUåˆ©ç”¨å¯èƒ½ãªã‚‰GPUï¼‰
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    print(f"âœ“ ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº† (ãƒ‡ãƒã‚¤ã‚¹: {device})")
    
    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆå¤‰æ›ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒ16kHzã‚’æœŸå¾…ã—ã¦ã„ã‚‹å ´åˆï¼‰
    if sample_rate != 16000:
        print(f"\nğŸ”„ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆå¤‰æ›ä¸­: {sample_rate} Hz â†’ 16000 Hz")
        from scipy import signal
        audio_data = signal.resample(audio_data, int(len(audio_data) * 16000 / sample_rate))
        sample_rate = 16000
    
    print("\nğŸ” æ„Ÿæƒ…åˆ†æä¸­...")
    
    # ç‰¹å¾´æŠ½å‡º
    inputs = feature_extractor(
        audio_data, 
        sampling_rate=sample_rate,
        return_tensors="pt",
        padding=True
    )
    
    # æ¨è«–
    with torch.no_grad():
        inputs = {key: val.to(device) for key, val in inputs.items()}
        logits = model(**inputs).logits
        probabilities = torch.nn.functional.softmax(logits, dim=-1)
    
    # ãƒ¢ãƒ‡ãƒ«ã®å…ƒã®æ„Ÿæƒ…ãƒ©ãƒ™ãƒ«
    model_emotions = ['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad']
    
    # ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’å–å¾—
    raw_results = {}
    for idx, emotion in enumerate(model_emotions):
        prob = probabilities[0][idx].item()
        raw_results[emotion] = prob
    
    # ã‚«ã‚¹ã‚¿ãƒ æ„Ÿæƒ…ãƒãƒƒãƒ”ãƒ³ã‚°
    # å«Œæ‚ªã‚’æ€’ã‚Šã¨ææ€–ã«åˆ†æ•£ï¼ˆ60%æ€’ã‚Šã€40%ææ€–ï¼‰
    disgust_prob = raw_results['disgust']
    
    # å–œã³ã‚’è¨ˆç®—ï¼ˆè½ã¡ç€ãã¨å¹¸ç¦ã®ä¸­é–“ã¨ã—ã¦ã€ä¸¡æ–¹ã®å½±éŸ¿ã‚’å—ã‘ã‚‹ï¼‰
    # å–œã³ = (è½ã¡ç€ã * 0.3 + å¹¸ç¦ * 0.7) ã®é‡ã¿ä»˜ãå¹³å‡
    joyful_prob = raw_results['calm'] * 0.3 + raw_results['happy'] * 0.7
    
    # æœ€çµ‚çš„ãªæ„Ÿæƒ…ã®ç¢ºç‡ã‚’è¨ˆç®—
    results = {
        'angry': raw_results['angry'] + disgust_prob * 0.6,  # å«Œæ‚ªã®60%ã‚’æ€’ã‚Šã«
        'calm': raw_results['calm'] * 0.7,  # è½ã¡ç€ãã‚’èª¿æ•´ï¼ˆå–œã³ã«ä¸€éƒ¨ä½¿ç”¨ï¼‰
        'joyful': joyful_prob,  # æ–°ã—ã„æ„Ÿæƒ…ã€Œå–œã³ã€
        'fearful': raw_results['fearful'] + disgust_prob * 0.4,  # å«Œæ‚ªã®40%ã‚’ææ€–ã«
        'happy': raw_results['happy'] * 0.3,  # å¹¸ç¦ã‚’èª¿æ•´ï¼ˆå–œã³ã«ä¸€éƒ¨ä½¿ç”¨ï¼‰
        'neutral': raw_results['neutral'],
        'sad': raw_results['sad']
    }
    
    # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
    emotions_jp = {
        'angry': 'æ€’ã‚Š',
        'calm': 'è½ã¡ç€ã',
        'joyful': 'å–œã³',
        'fearful': 'ææ€–',
        'happy': 'å¹¸ç¦',
        'neutral': 'ä¸­ç«‹',
        'sad': 'æ‚²ã—ã¿'
    }
    
    # çµæœè¡¨ç¤º
    print("\n" + "=" * 60)
    print("ã€æ„Ÿæƒ…åˆ†æçµæœã€‘")
    print("=" * 60)
    
    # æ„Ÿæƒ…ã®è¡¨ç¤ºé †åºã‚’å›ºå®šï¼ˆå¹¸ç¦ã€å–œã³ã€è½ã¡ç€ãã€ä¸­ç«‹ã€æ‚²ã—ã¿ã€æ€’ã‚Šã€ææ€–ï¼‰
    emotion_order = ['happy', 'joyful', 'calm', 'neutral', 'sad', 'angry', 'fearful']
    
    for emotion in emotion_order:
        prob = results[emotion]
        emotion_jp = emotions_jp[emotion]
        bar = "â–ˆ" * int(prob * 40)
        print(f"{emotion_jp:8s} ({emotion:8s}): {bar} {prob:.3f}")
    
    # æœ€ã‚‚é«˜ã„æ„Ÿæƒ…
    dominant_emotion = max(results, key=results.get)
    dominant_emotion_jp = emotions_jp[dominant_emotion]
    print(f"\nğŸ¯ æœ€ã‚‚å¼·ã„æ„Ÿæƒ…: {dominant_emotion_jp} ({dominant_emotion})")
    print(f"   ä¿¡é ¼åº¦: {results[dominant_emotion]:.1%}")
    
    # ä¿¡é ¼åº¦ã®è©•ä¾¡
    if results[dominant_emotion] > 0.7:
        print("   è©•ä¾¡: é«˜ã„ä¿¡é ¼åº¦ âœ…")
    elif results[dominant_emotion] > 0.4:
        print("   è©•ä¾¡: ä¸­ç¨‹åº¦ã®ä¿¡é ¼åº¦ âš ï¸")
    else:
        print("   è©•ä¾¡: ä½ã„ä¿¡é ¼åº¦ï¼ˆè¤‡æ•°ã®æ„Ÿæƒ…ãŒæ··åœ¨ï¼‰âš ï¸")
    
    print("=" * 60)
    
except Exception as e:
    print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    print("\nä»£æ›¿æ¡ˆ: Vokaturiã‚’ä½¿ç”¨ã—ã¾ã™...\n")
    
    # Vokaturiã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    import Vokaturi
    
    dll_path = os.path.join(os.path.dirname(__file__), "OpenVokaturi-4-0-win64.dll")
    Vokaturi.load(dll_path)
    
    # Vokaturiã§åˆ†æ
    voice = Vokaturi.Voice(sample_rate, len(audio_data), True)
    buffer_pointer = audio_data.ctypes.data_as(ctypes.POINTER(ctypes.c_double))
    voice.fill_float64array(len(audio_data), buffer_pointer)
    
    quality = Vokaturi.Quality()
    emotion_probabilities = Vokaturi.EmotionProbabilities()
    voice.extract(quality, emotion_probabilities)
    
    print("ã€Vokaturiåˆ†æçµæœã€‘")
    print(f"ä¸­ç«‹: {emotion_probabilities.neutrality:.3f}")
    print(f"å¹¸ç¦: {emotion_probabilities.happiness:.3f}")
    print(f"æ‚²ã—ã¿: {emotion_probabilities.sadness:.3f}")
    print(f"æ€’ã‚Š: {emotion_probabilities.anger:.3f}")
    print(f"ææ€–: {emotion_probabilities.fear:.3f}")
    
    voice.destroy()
